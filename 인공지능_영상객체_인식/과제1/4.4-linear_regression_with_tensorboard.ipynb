{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 예제: Tensorboard를 이용한 학습과정 시각화\n",
        "\n",
        "\n",
        "---\n",
        "이 실습파일은 선형회귀 실습 예제 파일을 수정한 것입니다. tf.summary API를 추가한 것 외에는 동일합니다.\n",
        "\n",
        "Tensorflow 라이브러리를 import합니다.\n",
        "\n",
        "**Note-1:** 2025.03.18 PC로 log file을 다운로드해서 tensorboard를 실행하는 방식에서 colab 자체에서 tensorboard를 실행하는 방식으로 변환함.\n",
        "\n",
        "**Note-2:** 2025.03.18 tf version=2.18.0"
      ],
      "metadata": {
        "id": "8ma1ar8fRDuW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GJauoXBRCgw"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "weight W, bias B 변수를 선언하고, 선형회귀 모델을 정의합니다."
      ],
      "metadata": {
        "id": "vgAX320xDU-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 선형회귀 모델(Wx + b)과 tf.Variable을 선언합니다.\n",
        "W = tf.Variable(tf.random.normal(shape=[1]))\n",
        "b = tf.Variable(tf.random.normal(shape=[1]))\n",
        "\n",
        "@tf.function\n",
        "def linear_model(x):\n",
        "  return W*x + b\n"
      ],
      "metadata": {
        "id": "wM4hvpMiRTm4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "예측값과 목적값과의 차이를 나타내는 손실함수를 평균제곱오차로 정의합니다."
      ],
      "metadata": {
        "id": "QfKaymklDgY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 손실 함수를 정의합니다.\n",
        "# MSE 손실함수 \\mean{(y' - y)^2}\n",
        "@tf.function\n",
        "def mse_loss(y_pred, y):\n",
        "  return tf.reduce_mean(tf.square(y_pred - y))\n"
      ],
      "metadata": {
        "id": "siCIgClhRXm-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "최적화 함수로 그라디언트 디센트 옵티마이저를 정의합니다."
      ],
      "metadata": {
        "id": "44MHEbxsDzb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 최적화를 위한 그라디언트 디센트 옵티마이저를 정의합니다.\n",
        "optimizer = tf.optimizers.SGD(0.01)\n"
      ],
      "metadata": {
        "id": "NKZl6T0FRpP8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "여기서부터는 앞에서 실습한 코드와 조금 달라지는데,\n",
        "\n",
        "1.   먼저 colab의 tensorboard extension을 불러오고,\n",
        "2.   tf.summary 라이브러리의 create_file_writer함수로 스텝별 손실함수 값을 저장할 폴더 경로를 지정합니다."
      ],
      "metadata": {
        "id": "5opSPi6MD24H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "\n",
        "# 텐서보드 summary 정보들을 저장할 폴더 경로를 설정합니다.\n",
        "summary_writer = tf.summary.create_file_writer('./tensorboard_log')\n"
      ],
      "metadata": {
        "id": "N13cwD1LRt-m"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "여기에서 W와 b값의 최적화 업데이트를 위한 함수를 정의하는데, 이때 이 함수가 한번 실행될 때마다, tf.summary 라이브러리의 scalar함수를 사용해서 loss 손실함수 값이 기록되도록 코드에 추가했습니다.\n",
        "그 아랫부분은 앞서 실습했던 선형회귀 부분과 동일하게, W와 b에 대해 loss값을 미분하고, 이 미분값을 적용해서 W와 b값을 업데이트 하게 됩니다.\n"
      ],
      "metadata": {
        "id": "CAPD7stREHrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 최적화를 위한 학습과 학습결과를 Tensorboar에 기록할 함수를 정의합니다.\n",
        "@tf.function\n",
        "def train_step(x, y):\n",
        "  with tf.GradientTape() as tape:\n",
        "    y_pred = linear_model(x)\n",
        "    loss = mse_loss(y_pred, y)\n",
        "  with summary_writer.as_default():\n",
        "    tf.summary.scalar('loss', loss, step=optimizer.iterations)\n",
        "  gradients = tape.gradient(loss, [W, b])\n",
        "  optimizer.apply_gradients(zip(gradients, [W, b]))\n"
      ],
      "metadata": {
        "id": "iPy_-I0VRxDV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 아래 부분은 선형회귀와 동일하게, 트레이닝 데이터를 준비하고, 경사하강법을 1000번 실행한 후에, 학습이 잘 되었는지 테스트 데이터를 넣어서 정확도를 측정해보는 것입니다."
      ],
      "metadata": {
        "id": "MBsZJ9ahEV49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 트레이닝을 위한 입력값과 출력값을 준비합니다.\n",
        "x_train = [1, 2, 3, 4]\n",
        "y_train = [2, 4, 6, 8]\n"
      ],
      "metadata": {
        "id": "zqz7tSGYSN79"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 경사하강법을 1000번 수행합니다.\n",
        "# 1000번 수행마다, 'tensorboard_log' 폴더에 loss값이 log파일에 기록됩니다.\n",
        "for i in range(1000):\n",
        "  train_step(x_train, y_train)\n"
      ],
      "metadata": {
        "id": "7rVvw2C4SOuV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트를 위한 입력값을 준비합니다.\n",
        "x_test = [3.5, 5, 5.5, 6]\n",
        "# 테스트 데이터를 이용해 학습된 선형회귀 모델이 데이터의 경향성(y=2x)을 잘 학습했는지 측정합니다.\n",
        "# 예상되는 참값 : [7, 10, 11, 12]\n",
        "print(linear_model(x_test).numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGmdlcbUSRmB",
        "outputId": "ce493139-1dae-4068-95f4-9963740aa923"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 7.0024896 10.009158  11.011381  12.013604 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**colab tensorboard extenstion을 실행해서, 학습과정을 시각화합니다.**"
      ],
      "metadata": {
        "id": "CwVKWRDbMU1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir ./tensorboard_log"
      ],
      "metadata": {
        "id": "yPvdIPVSMS_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**tensorboard를 실행해서, 학습과정을 시각화하기 위해서는 아래의 과정을 따라합니다.(2025.03.18시점 아래 방법은 동작하지 않음-PC에 설치된 tensorflow version이 log file format을 제대로 읽지 못함)**\n",
        "\n",
        "1.   'tensorboard_log' 폴더에 있는 log file을 본인의 PC에 다운로드 받습니다.(코랩에서 왼쪽에 보이는 파일 폴더를 누르면, 현재 디렉터리의 폴더들과 파일들이 보이는데, 여기에서 tensorboard_log 폴더를 눌러보면 events로 시작되는 로그 파일이 보입니다.\n",
        "이 로그파일을 더블클릭해서 보면 알겠지만, 학습을 진행하면서 loss값이 계속 누적되어서 저장되어 있는 것이 보입니다.)\n",
        "2.   windows의 \"명령 프롬프트\"나 \"powershell\"앱을 log file이 위치한 곳에서 띄워서, '$tensorboard --logdir=./' 명령을 실행합니다.\n",
        "3. Chrome이나 IE창을 띄워서 'http://localhost:6006'을 주소창에 입력합니다.\n",
        "\n",
        "학습 과정을 그래프로 볼 수 있는데, loss값들이 학습이 진행되면서 줄어드는 것을 알 수 있고, 제대로 학습이 되었다는 것을 알 수가 있습니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "nQrroIlbTGbj"
      }
    }
  ]
}